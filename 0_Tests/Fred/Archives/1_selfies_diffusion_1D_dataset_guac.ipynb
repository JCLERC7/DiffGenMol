{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimised diffusion models 1D\n",
    "Notebook created by Frédéric Charbonnier & Joel Clerc as part of the Master IS research project \"Learning to generate\n",
    "molecules\".  \n",
    "This notebook uses [Phil Wang's GitHub](https://github.com/lucidrains/denoising-diffusion-pytorch) to implement a conditional and unconditional DDMPs.\n",
    "Inspired by [Nathan C. Frey's Tutorial](https://ncfrey.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion model 1D with classifier guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch_1d.py\n",
    "# Classifier guidance added (inspired by https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/classifier_free_guidance.py)\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from denoising_diffusion_pytorch.version import __version__\n",
    "\n",
    "# constants\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# helpers functions\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def has_int_squareroot(num):\n",
    "    return (math.sqrt(num) ** 2) == num\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "def convert_image_to_fn(img_type, image):\n",
    "    if image.mode != img_type:\n",
    "        return image.convert(img_type)\n",
    "    return image\n",
    "\n",
    "# normalization functions\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# classifier free guidance functions\n",
    "\n",
    "def uniform(shape, device):\n",
    "    return torch.zeros(shape, device = device).float().uniform_(0, 1)\n",
    "\n",
    "def prob_mask_like(shape, prob, device):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "    else:\n",
    "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n",
    "\n",
    "# small helper modules\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv1d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out = None):\n",
    "    return nn.Conv1d(dim, default(dim_out, dim), 4, 2, 1)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "# sinusoidal positional embeds\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "# building block modules\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, classes_emb_dim = None, groups = 8):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(int(time_emb_dim) + int(classes_emb_dim), dim_out * 2)\n",
    "        ) if exists(time_emb_dim) or exists(classes_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None, class_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        #if exists(self.mlp) and exists(time_emb):\n",
    "        #    time_emb = self.mlp(time_emb)\n",
    "        #    time_emb = rearrange(time_emb, 'b c -> b c 1')\n",
    "        #    scale_shift = time_emb.chunk(2, dim = 1)\n",
    "        \n",
    "        if exists(self.mlp) and (exists(time_emb) or exists(class_emb)):\n",
    "            cond_emb = tuple(filter(exists, (time_emb, class_emb)))\n",
    "            cond_emb = torch.cat(cond_emb, dim = -1)\n",
    "            cond_emb = self.mlp(cond_emb)\n",
    "            cond_emb = rearrange(cond_emb, 'b c -> b c 1')\n",
    "            scale_shift = cond_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale        \n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c n -> b (h c) n', h = self.heads)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv1d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b (h d) n')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# model\n",
    "\n",
    "class Unet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_classes,\n",
    "        cond_drop_prob = 0.5,\n",
    "        init_dim = None,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels = 3,\n",
    "        self_condition = False,\n",
    "        resnet_block_groups = 8,\n",
    "        learned_variance = False,\n",
    "        learned_sinusoidal_cond = False,\n",
    "        random_fourier_features = False,\n",
    "        learned_sinusoidal_dim = 16,\n",
    "        sinusoidal_pos_emb_theta = 10000,\n",
    "        attn_dim_head = 32,\n",
    "        attn_heads = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # classifier free guidance stuff\n",
    "\n",
    "        self.cond_drop_prob = cond_drop_prob\n",
    "\n",
    "        # determine dimensions\n",
    "\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv1d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n",
    "\n",
    "        if self.random_or_learned_sinusoidal_cond:\n",
    "            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n",
    "            fourier_dim = learned_sinusoidal_dim + 1\n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)\n",
    "            fourier_dim = dim\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "         # class embeddings\n",
    "\n",
    "        self.classes_emb = nn.Embedding(num_classes, dim)\n",
    "        self.null_classes_emb = nn.Parameter(torch.randn(dim))\n",
    "\n",
    "        classes_dim = dim * 4\n",
    "\n",
    "        self.classes_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, classes_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(classes_dim, classes_dim)\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass(dim_in, dim_in, time_emb_dim = time_dim, classes_emb_dim = classes_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim = time_dim, classes_emb_dim = classes_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv1d(dim_in, dim_out, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim, classes_emb_dim = classes_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim, dim_head = attn_dim_head, heads = attn_heads)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim, classes_emb_dim = classes_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim, classes_emb_dim = classes_dim),\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim, classes_emb_dim = classes_dim),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv1d(dim_out, dim_in, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim, classes_emb_dim = classes_dim)\n",
    "        self.final_conv = nn.Conv1d(dim, self.out_dim, 1)\n",
    "    \n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        *args,\n",
    "        cond_scale = 1.,\n",
    "        rescaled_phi = 0.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        logits = self.forward(*args, cond_drop_prob = 0., **kwargs)\n",
    "\n",
    "        if cond_scale == 1:\n",
    "            return logits\n",
    "\n",
    "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n",
    "        scaled_logits = null_logits + (logits - null_logits) * cond_scale\n",
    "\n",
    "        if rescaled_phi == 0.:\n",
    "            return scaled_logits\n",
    "\n",
    "        std_fn = partial(torch.std, dim = tuple(range(1, scaled_logits.ndim)), keepdim = True)\n",
    "        rescaled_logits = scaled_logits * (std_fn(logits) / std_fn(scaled_logits))\n",
    "\n",
    "        return rescaled_logits * rescaled_phi + scaled_logits * (1. - rescaled_phi)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x, \n",
    "            time, \n",
    "            classes,\n",
    "            cond_drop_prob = None,\n",
    "            x_self_cond = None):\n",
    "        \n",
    "        batch, device = x.shape[0], x.device\n",
    "\n",
    "        cond_drop_prob = default(cond_drop_prob, self.cond_drop_prob)\n",
    "\n",
    "        # derive condition, with condition dropout for classifier free guidance        \n",
    "\n",
    "        classes_emb = self.classes_emb(classes)\n",
    "\n",
    "        if cond_drop_prob > 0:\n",
    "            keep_mask = prob_mask_like((batch,), 1 - cond_drop_prob, device = device)\n",
    "            null_classes_emb = repeat(self.null_classes_emb, 'd -> b d', b = batch)\n",
    "\n",
    "            classes_emb = torch.where(\n",
    "                rearrange(keep_mask, 'b -> b 1'),\n",
    "                classes_emb,\n",
    "                null_classes_emb\n",
    "            )\n",
    "\n",
    "        c = self.classes_mlp(classes_emb)\n",
    "\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim = 1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t, c)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t, c)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t, c)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t, c)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t, c)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t, c)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t, c)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# gaussian diffusion trainer class\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "class GaussianDiffusion1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        seq_length,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'cosine',\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # calculate loss weight\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            loss_weight = torch.ones_like(snr)\n",
    "        elif objective == 'pred_x0':\n",
    "            loss_weight = snr\n",
    "        elif objective == 'pred_v':\n",
    "            loss_weight = snr / (snr + 1)\n",
    "\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "\n",
    "        # whether to autonormalize\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, classes, cond_scale = 6., rescaled_phi = 0.7, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n",
    "        #model_output = self.model(x, t, x_self_cond)\n",
    "        model_output = self.model.forward_with_cond_scale(x, t, classes, cond_scale = cond_scale, rescaled_phi = rescaled_phi)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, classes, cond_scale, rescaled_phi, x_self_cond = None, clip_denoised = True):\n",
    "        #preds = self.model_predictions(x, t, x_self_cond)\n",
    "        preds = self.model_predictions(x, t, classes, cond_scale, rescaled_phi)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, classes, cond_scale = 6., rescaled_phi = 0.7, x_self_cond = None, clip_denoised = True):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "        #model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = clip_denoised)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, classes = classes, cond_scale = cond_scale, rescaled_phi = rescaled_phi, x_self_cond = x_self_cond, clip_denoised = clip_denoised)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, classes, shape, cond_scale = 6., rescaled_phi = 0.7):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            #img, x_start = self.p_sample(img, t, self_cond)\n",
    "            img, x_start = self.p_sample(img, t, classes, cond_scale, rescaled_phi, self_cond)\n",
    "\n",
    "        img = self.unnormalize(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, classes, shape, cond_scale = 6., rescaled_phi = 0.7, clip_denoised = True):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            #pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = clip_denoised)\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, classes, cond_scale = cond_scale, rescaled_phi = rescaled_phi, x_self_cond=self_cond, clip_x_start = clip_denoised)\n",
    "\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        img = self.unnormalize(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    #def sample(self, classes, cond_scale = 6., rescaled_phi = 0.7, batch_size = 16):\n",
    "    def sample(self, classes, cond_scale = 6., rescaled_phi = 0.7):\n",
    "        batch_size, seq_length, channels = classes.shape[0], self.seq_length, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn(classes, (batch_size, channels, seq_length), cond_scale, rescaled_phi)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, classes, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, classes, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, classes, noise = None):\n",
    "        b, c, n = x_start.shape\n",
    "        #b, n = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        #model_out = self.model(x, t, x_self_cond)\n",
    "        model_out = self.model(x, t, classes, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, n, device, seq_length, = *img.shape, img.device, self.seq_length\n",
    "        #b, n, device, seq_length, = *img.shape, img.device, self.seq_length\n",
    "        assert n == seq_length, f'seq length must be {seq_length}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecules functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --pre deepchem[torch]\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import RDLogger  \n",
    "from rdkit.Chem.Fingerprints.FingerprintMols import FingerprintMol\n",
    "from rdkit.DataStructs import FingerprintSimilarity\n",
    "RDLogger.DisableLog('rdApp.*')  # suppress error messages\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Helper\n",
    "def preprocess_smiles(smiles):\n",
    " return sf.encoder(smiles)  \n",
    "\n",
    "def keys_int(symbol_to_int):\n",
    "  d={}\n",
    "  i=0\n",
    "  for key in symbol_to_int.keys():\n",
    "    d[i]=key\n",
    "    i+=1\n",
    "  return d\n",
    "\n",
    "def by_max_size(values, max_size):\n",
    "    return [value for value in values if len(value) <= max_size]\n",
    "\n",
    "# Load smiles dataset\n",
    "def get_smiles(nb_sample = 2500, max_smiles_size = 30):\n",
    "  # Download from MolNet\n",
    "  dataset = load_dataset(\"jxie/guacamol\")\n",
    "  all_smiles = [data['text'] for data in dataset['train']]\n",
    "  smiles_limited_by_size = by_max_size(all_smiles, max_smiles_size)\n",
    "  print(len(smiles_limited_by_size))\n",
    "  df = pd.DataFrame(data={'smiles': smiles_limited_by_size})\n",
    "  data = df[['smiles']].sample(nb_sample, random_state=42)\n",
    "  return data['smiles']\n",
    "\n",
    "def smiles_to_selfies(smiles):\n",
    "   sf.set_semantic_constraints()  # reset constraints\n",
    "   constraints = sf.get_semantic_constraints()\n",
    "   constraints['?'] = 5\n",
    "   sf.set_semantic_constraints(constraints)\n",
    "   selfies_list = np.asanyarray(smiles.apply(preprocess_smiles))\n",
    "   return selfies_list\n",
    "\n",
    "def selfies_to_continous_mols(selfies):\n",
    "   selfies_alphabet = sf.get_alphabet_from_selfies(selfies)\n",
    "   selfies_alphabet.add('[nop]')  # Add the \"no operation\" symbol as a padding character\n",
    "   selfies_alphabet.add('.') \n",
    "   selfies_alphabet = list(sorted(selfies_alphabet))\n",
    "   largest_selfie_len = max(sf.len_selfies(s) for s in selfies)\n",
    "   symbol_to_int = dict((c, i) for i, c in enumerate(selfies_alphabet))\n",
    "   int_mol=keys_int(symbol_to_int)\n",
    "   onehots=sf.batch_selfies_to_flat_hot(selfies, symbol_to_int,largest_selfie_len)\n",
    "   input_tensor = torch.tensor(onehots, dtype=torch.float32)\n",
    "   noise_tensor = torch.rand(input_tensor.shape, dtype=torch.float32)\n",
    "   dequantized_onehots = input_tensor + noise_tensor\n",
    "   continous_mols = (dequantized_onehots - dequantized_onehots.min()) / (dequantized_onehots.max() - dequantized_onehots.min())\n",
    "   return continous_mols, selfies_alphabet, largest_selfie_len, int_mol, dequantized_onehots.min(), dequantized_onehots.max()\n",
    "\n",
    "def mols_continous_to_selfies(continous_mols, selfies_alphabet, largest_selfie_len, int_mol, dequantized_onehots_min, dequantized_onehots_max):\n",
    "   denormalized_data = continous_mols * (dequantized_onehots_max - dequantized_onehots_min) + dequantized_onehots_min\n",
    "   quantized_data = torch.floor(denormalized_data)\n",
    "   quantized_data = torch.clip(quantized_data, 0, 1)\n",
    "   mols_list = quantized_data.cpu().int().numpy().tolist()\n",
    "   #print(mols_list)\n",
    "   for mol in mols_list:\n",
    "    for i in range(largest_selfie_len):\n",
    "        row = mol[len(selfies_alphabet) * i: len(selfies_alphabet) * (i + 1)]\n",
    "        if all(elem == 0 for elem in row):\n",
    "            mol[len(selfies_alphabet) * (i+1) - 1] = 1\n",
    "   selfies = sf.batch_flat_hot_to_selfies(mols_list, int_mol)\n",
    "   return selfies\n",
    "\n",
    "def selfies_to_mols(selfies_to_convert):\n",
    "  valid_count = 0\n",
    "  valid_selfies, invalid_selfies = [], []\n",
    "  mols = []\n",
    "  for idx, selfies in enumerate(selfies_to_convert):\n",
    "    try:\n",
    "      if Chem.MolFromSmiles(sf.decoder(selfies_to_convert[idx]), sanitize=True) is not None:\n",
    "          valid_count += 1\n",
    "          valid_selfies.append(selfies)\n",
    "          mols.append(Chem.MolFromSmiles(sf.decoder(selfies_to_convert[idx])))\n",
    "      else:\n",
    "        invalid_selfies.append(selfies)\n",
    "    except Exception:\n",
    "      pass\n",
    "  return mols, valid_selfies, valid_count\n",
    "\n",
    "def get_mols_properties(mols, prop):\n",
    "  if prop == \"Weight\":\n",
    "    molsWt = [Chem.Descriptors.MolWt(mol) for mol in mols]\n",
    "    return molsWt\n",
    "  elif prop == \"LogP\":\n",
    "    molsLogP = [Chem.Descriptors.MolLogP(mol) for mol in mols]\n",
    "    return molsLogP\n",
    "  elif prop == \"QED\":\n",
    "    molsQED = [Chem.QED.default(mol) for mol in mols]\n",
    "    return molsQED\n",
    "\n",
    "# Calculate similarity\n",
    "def tanimoto_similarity(database_mols, query_mol):\n",
    "  \"\"\"Compare generated molecules to database by Tanimoto similarity.\"\"\"\n",
    "  # convert Mol to datastructure type\n",
    "  fps = [FingerprintMol(m) for m in database_mols]\n",
    "  \n",
    "  # set a query molecule to compare against database\n",
    "  query = FingerprintMol(query_mol)\n",
    "  \n",
    "  similarities = []\n",
    "  \n",
    "  # loop through to find Tanimoto similarity\n",
    "  for idx, f in enumerate(fps):\n",
    "      # tuple: (idx, similarity)\n",
    "      similarities.append((idx, FingerprintSimilarity(query, f)))\n",
    "  \n",
    "  # sort sim using the similarities\n",
    "  similarities.sort(key=lambda x:x[1], reverse=True)\n",
    "  \n",
    "  return similarities\n",
    "\n",
    "def discretize_continuous_values(values, num_classes):\n",
    "    min_bound = min(values)\n",
    "    max_bound = max(values)\n",
    "    intervals = np.linspace(min_bound, max_bound, num_classes)\n",
    "    assigned_classes = np.digitize(values, intervals, right=True)\n",
    "    return torch.from_numpy(assigned_classes), intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x286826e7850>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test get_selfies_list\n",
    "original_selfies = smiles_to_selfies(get_smiles())\n",
    "mol = Chem.MolFromSmiles(sf.decoder(original_selfies[2]))\n",
    "mol_img = Chem.Draw.MolToImage(mol,size=(300,300))\n",
    "plt.imshow(mol_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Original Selfies -> continous_mols -> Recalculate Selfies\n",
    "continous_mols, selfies_alphabet, largest_selfie_len, int_mol, dequantized_onehots_min, dequantized_onehots_max = selfies_to_continous_mols(original_selfies)\n",
    "recalculate_selfies_test = mols_continous_to_selfies(continous_mols, selfies_alphabet, largest_selfie_len, int_mol, dequantized_onehots_min, dequantized_onehots_max)\n",
    "mols_test, valid_selfies_list, valid_count = selfies_to_mols(recalculate_selfies_test)\n",
    "print('%.2f' % (valid_count / len(mols_test)*100),  '% of generated samples are valid molecules.')\n",
    "\n",
    "smiles_label = [Chem.MolToSmiles(mol) for mol in mols_test[:9]]\n",
    "img = Chem.Draw.MolsToGridImage(mols_test[:9], molsPerRow=3, subImgSize=(200,200), returnPNG=False)\n",
    "img.save('results/test-functions.png')   \n",
    "\n",
    "#Similarity\n",
    "original_mols, _, _ = selfies_to_mols(original_selfies)\n",
    "tanimoto_scores = tanimoto_similarity(original_mols, mols_test[2])\n",
    "print(\"Top3 similarity score:\")\n",
    "for idx, ts in tanimoto_scores[:3]:\n",
    "    print(round(ts, 3))\n",
    "\n",
    "print(\"Weight:\")\n",
    "print(get_mols_properties([mols_test[2]],\"Weight\")[0])\n",
    "\n",
    "print(\"LogP:\")\n",
    "print(get_mols_properties([mols_test[2]],\"LogP\")[0])\n",
    "\n",
    "print(\"QED:\")\n",
    "print(get_mols_properties([mols_test[2]],\"QED\")[0])\n",
    "\n",
    "mol_img = Chem.Draw.MolToImage(mols_test[2],size=(300,300))\n",
    "plt.imshow(mol_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of custom dataset\n",
    "class Dataset1D(Dataset):\n",
    "    def __init__(self, x: Tensor, y: Tensor):\n",
    "        super().__init__()\n",
    "        self.x = x.clone()\n",
    "        self.y = y.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(9)\n",
      "dataset size:\n",
      "torch.Size([2500, 1312])\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "#original_selfies = smiles_to_selfies(get_smiles())\n",
    "dataset, selfies_alphabet, largest_selfie_len, int_mol, dequantized_onehots_min, dequantized_onehots_max = selfies_to_continous_mols(original_selfies)\n",
    "# original mols for similarity calculation\n",
    "original_mols, _, _ = selfies_to_mols(original_selfies)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# properties\n",
    "continuous_properties = get_mols_properties(original_mols,\"LogP\")\n",
    "classes, intervals = discretize_continuous_values(continuous_properties, num_classes)\n",
    "\n",
    "seq_length = dataset.shape[1]\n",
    "channels = 1\n",
    "batch_size = 16\n",
    "\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "\n",
    "print(\"dataset size:\")\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.3649999999999998\n",
      "7.131400000000009\n",
      "tensor([5, 3, 5,  ..., 5, 6, 4])\n",
      "[-3.365      -2.19873333 -1.03246667  0.1338      1.30006667  2.46633333\n",
      "  3.6326      4.79886667  5.96513333  7.1314    ]\n"
     ]
    }
   ],
   "source": [
    "print(min(continuous_properties))\n",
    "print(max(continuous_properties))\n",
    "print(classes)\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_and_sample_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset1D(dataset[:, None, :].float(), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Unet1D(\n",
    "    dim = 16,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_classes = num_classes,\n",
    "    channels = channels,\n",
    "    cond_drop_prob = 0.5\n",
    "    #cond_drop_prob = 0.\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    model,\n",
    "    seq_length = seq_length,\n",
    "    timesteps = 300\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0968, 0.1993, 0.2496,  ..., 0.1288, 0.2329, 0.5377]],\n",
      "\n",
      "        [[0.4624, 0.1173, 0.1953,  ..., 0.4720, 0.3942, 0.8136]],\n",
      "\n",
      "        [[0.2002, 0.0972, 0.2368,  ..., 0.0633, 0.3505, 0.8874]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2694, 0.0050, 0.4610,  ..., 0.0893, 0.0899, 0.7471]],\n",
      "\n",
      "        [[0.4429, 0.1876, 0.1658,  ..., 0.4600, 0.2842, 0.7817]],\n",
      "\n",
      "        [[0.2688, 0.3177, 0.2876,  ..., 0.2589, 0.0182, 0.9548]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 7, 4, 7, 5, 5, 2, 6, 6, 5, 4, 6, 5, 7, 6, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "Loss: 1.205939531326294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb85e177d9c84ef29d3608ce25806e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00 % of generated samples are valid molecules.\n",
      "Similarity\n",
      "----------\n",
      "Smiles: C#N.N\n",
      "Top3 similarity score:\n",
      "0.091\n",
      "0.091\n",
      "0.067\n",
      "Smiles: C.C.C.C.C#C.N\n",
      "Top3 similarity score:\n",
      "0.143\n",
      "0.143\n",
      "0.091\n",
      "Smiles: C.C#N.N\n",
      "Top3 similarity score:\n",
      "0.091\n",
      "0.091\n",
      "0.067\n",
      "Smiles: C.C.N\n",
      "Top3 similarity score:\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "Smiles: N.N\n",
      "Top3 similarity score:\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "Smiles: C.C.N\n",
      "Top3 similarity score:\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "Smiles: C.C.N.N\n",
      "Top3 similarity score:\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "Smiles: C.N\n",
      "Top3 similarity score:\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "Smiles: N.N\n",
      "Top3 similarity score:\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "tensor([[[0.0135, 0.2220, 0.1627,  ..., 0.1501, 0.0192, 0.8730]],\n",
      "\n",
      "        [[0.2531, 0.3632, 0.2693,  ..., 0.2867, 0.2548, 0.7135]],\n",
      "\n",
      "        [[0.3914, 0.0562, 0.4933,  ..., 0.3116, 0.4534, 0.7981]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2663, 0.0414, 0.1443,  ..., 0.2056, 0.2078, 0.6110]],\n",
      "\n",
      "        [[0.4202, 0.1878, 0.0887,  ..., 0.3401, 0.3137, 0.6937]],\n",
      "\n",
      "        [[0.0892, 0.1081, 0.0309,  ..., 0.4551, 0.2725, 0.9985]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 7, 5, 5, 7, 6, 6, 5, 7, 7, 4, 5, 6, 6, 3, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[7.7679e-02, 4.6700e-01, 1.7425e-01,  ..., 2.8143e-01,\n",
      "          6.9866e-02, 7.0458e-01]],\n",
      "\n",
      "        [[2.9342e-01, 1.2385e-01, 3.8301e-01,  ..., 7.1771e-04,\n",
      "          4.9126e-01, 8.9931e-01]],\n",
      "\n",
      "        [[3.2843e-01, 2.7529e-01, 4.0494e-01,  ..., 4.8276e-01,\n",
      "          8.6042e-02, 5.5555e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.5155e-03, 2.6269e-02, 3.9118e-02,  ..., 4.4076e-01,\n",
      "          4.1402e-01, 5.3351e-01]],\n",
      "\n",
      "        [[4.8741e-01, 4.5008e-01, 2.6282e-01,  ..., 1.9425e-01,\n",
      "          1.1137e-01, 6.6335e-01]],\n",
      "\n",
      "        [[2.1050e-02, 3.9857e-01, 2.5881e-01,  ..., 6.1545e-02,\n",
      "          5.4056e-03, 7.4791e-01]]], device='cuda:0')\n",
      "tensor([5, 6, 5, 7, 5, 5, 6, 6, 7, 7, 6, 6, 6, 5, 4, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1116, 0.0611, 0.3931,  ..., 0.3161, 0.3516, 0.8385]],\n",
      "\n",
      "        [[0.4451, 0.2676, 0.2171,  ..., 0.1642, 0.4436, 0.5594]],\n",
      "\n",
      "        [[0.0301, 0.1215, 0.3697,  ..., 0.2946, 0.1838, 0.8723]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3219, 0.2471, 0.1334,  ..., 0.3693, 0.4962, 0.8280]],\n",
      "\n",
      "        [[0.2999, 0.4856, 0.4826,  ..., 0.0884, 0.0900, 0.9016]],\n",
      "\n",
      "        [[0.2083, 0.0567, 0.2228,  ..., 0.2034, 0.0584, 0.5079]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 5, 5, 6, 6, 5, 7, 5, 5, 5, 4, 6, 9, 6, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1346, 0.3078, 0.0529,  ..., 0.1819, 0.2044, 0.8171]],\n",
      "\n",
      "        [[0.3599, 0.1220, 0.3330,  ..., 0.3161, 0.4301, 0.6366]],\n",
      "\n",
      "        [[0.1839, 0.2023, 0.3039,  ..., 0.2988, 0.4640, 0.9323]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0757, 0.2249, 0.3126,  ..., 0.4069, 0.4531, 0.6856]],\n",
      "\n",
      "        [[0.1824, 0.3395, 0.3719,  ..., 0.1553, 0.2309, 0.5231]],\n",
      "\n",
      "        [[0.4409, 0.4524, 0.1440,  ..., 0.4699, 0.2595, 0.8231]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 6, 7, 7, 5, 6, 6, 6, 7, 7, 8, 4, 6, 7, 4, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3374, 0.1261, 0.0864,  ..., 0.3551, 0.3033, 0.6112]],\n",
      "\n",
      "        [[0.4275, 0.0066, 0.1744,  ..., 0.3471, 0.2708, 0.9277]],\n",
      "\n",
      "        [[0.0375, 0.4323, 0.1729,  ..., 0.1063, 0.0238, 0.8474]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0466, 0.1459, 0.1755,  ..., 0.1717, 0.1041, 0.9131]],\n",
      "\n",
      "        [[0.0887, 0.3052, 0.3515,  ..., 0.4556, 0.0530, 0.7912]],\n",
      "\n",
      "        [[0.1837, 0.0409, 0.2040,  ..., 0.2238, 0.1346, 0.5645]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 9, 6, 5, 6, 5, 6, 5, 7, 4, 8, 6, 6, 6, 5, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4570, 0.3380, 0.4124,  ..., 0.1997, 0.2994, 0.5102]],\n",
      "\n",
      "        [[0.1015, 0.3875, 0.0282,  ..., 0.1843, 0.0503, 0.7048]],\n",
      "\n",
      "        [[0.2492, 0.0760, 0.0677,  ..., 0.1441, 0.2933, 0.9501]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2607, 0.4048, 0.3529,  ..., 0.4067, 0.1848, 0.8766]],\n",
      "\n",
      "        [[0.0685, 0.4674, 0.0123,  ..., 0.0803, 0.4948, 0.6662]],\n",
      "\n",
      "        [[0.4720, 0.4101, 0.1244,  ..., 0.3449, 0.3486, 0.5256]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 5, 4, 8, 3, 6, 4, 5, 6, 5, 6, 7, 7, 7, 7, 3], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0894, 0.3921, 0.1519,  ..., 0.0267, 0.4596, 0.5404]],\n",
      "\n",
      "        [[0.1703, 0.3851, 0.4738,  ..., 0.4778, 0.3408, 0.7998]],\n",
      "\n",
      "        [[0.3914, 0.4983, 0.1822,  ..., 0.2842, 0.3844, 0.8195]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2039, 0.0143, 0.3340,  ..., 0.2183, 0.4800, 0.8890]],\n",
      "\n",
      "        [[0.3333, 0.2585, 0.4654,  ..., 0.0695, 0.4435, 0.5024]],\n",
      "\n",
      "        [[0.2927, 0.0067, 0.2970,  ..., 0.1826, 0.1128, 0.8825]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 6, 5, 5, 6, 6, 7, 5, 7, 3, 6, 4, 7, 6, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0714, 0.2365, 0.1009,  ..., 0.2127, 0.1486, 0.8039]],\n",
      "\n",
      "        [[0.0765, 0.0711, 0.2039,  ..., 0.2608, 0.4580, 0.8981]],\n",
      "\n",
      "        [[0.3111, 0.4684, 0.2755,  ..., 0.1679, 0.4606, 0.7326]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1180, 0.1800, 0.2260,  ..., 0.3244, 0.0401, 0.5953]],\n",
      "\n",
      "        [[0.0057, 0.0400, 0.3152,  ..., 0.3369, 0.1594, 0.6825]],\n",
      "\n",
      "        [[0.3052, 0.4570, 0.1086,  ..., 0.3378, 0.4817, 0.9801]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 6, 7, 5, 6, 3, 5, 5, 4, 5, 7, 9, 5, 6, 5, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3013, 0.1705, 0.1921,  ..., 0.0622, 0.0445, 0.5859]],\n",
      "\n",
      "        [[0.2540, 0.0192, 0.3835,  ..., 0.3710, 0.4697, 0.5330]],\n",
      "\n",
      "        [[0.2871, 0.4466, 0.4906,  ..., 0.2992, 0.1684, 0.8792]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4471, 0.3226, 0.0132,  ..., 0.2637, 0.3143, 0.9536]],\n",
      "\n",
      "        [[0.2115, 0.1537, 0.0605,  ..., 0.2958, 0.1719, 0.7862]],\n",
      "\n",
      "        [[0.1087, 0.2782, 0.4188,  ..., 0.0299, 0.4077, 0.9214]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 7, 5, 5, 4, 6, 6, 5, 5, 6, 6, 4, 5, 7, 7, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4319, 0.3131, 0.2112,  ..., 0.4590, 0.4664, 0.5561]],\n",
      "\n",
      "        [[0.3464, 0.3340, 0.1467,  ..., 0.0160, 0.2082, 0.9091]],\n",
      "\n",
      "        [[0.3450, 0.0594, 0.1183,  ..., 0.1852, 0.2944, 0.5640]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2787, 0.3580, 0.1227,  ..., 0.1262, 0.2702, 0.6365]],\n",
      "\n",
      "        [[0.0540, 0.1307, 0.1494,  ..., 0.1695, 0.3161, 0.7634]],\n",
      "\n",
      "        [[0.1250, 0.2071, 0.2845,  ..., 0.2539, 0.4223, 0.7289]]],\n",
      "       device='cuda:0')\n",
      "tensor([7, 6, 5, 7, 4, 4, 7, 6, 4, 4, 5, 6, 4, 2, 6, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4589, 0.2689, 0.0053,  ..., 0.4045, 0.4427, 0.6319]],\n",
      "\n",
      "        [[0.2486, 0.4730, 0.0445,  ..., 0.1728, 0.4592, 0.9162]],\n",
      "\n",
      "        [[0.0861, 0.4651, 0.0036,  ..., 0.1183, 0.0372, 0.8085]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2322, 0.3666, 0.3519,  ..., 0.0326, 0.2241, 0.9668]],\n",
      "\n",
      "        [[0.2128, 0.1245, 0.4562,  ..., 0.0788, 0.0428, 0.6796]],\n",
      "\n",
      "        [[0.2807, 0.2430, 0.1523,  ..., 0.1797, 0.4529, 0.7423]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 7, 7, 6, 5, 6, 5, 6, 5, 7, 6, 6, 6, 6, 6, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3991, 0.1305, 0.3348,  ..., 0.3869, 0.3980, 0.6067]],\n",
      "\n",
      "        [[0.4166, 0.4882, 0.1824,  ..., 0.4405, 0.2427, 0.9955]],\n",
      "\n",
      "        [[0.0847, 0.3052, 0.0971,  ..., 0.2890, 0.1946, 0.8350]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0892, 0.1061, 0.4664,  ..., 0.3901, 0.2895, 0.9825]],\n",
      "\n",
      "        [[0.3678, 0.3522, 0.3500,  ..., 0.1425, 0.4459, 0.7038]],\n",
      "\n",
      "        [[0.2796, 0.3613, 0.3170,  ..., 0.2009, 0.4048, 0.5658]]],\n",
      "       device='cuda:0')\n",
      "tensor([8, 7, 6, 7, 6, 6, 8, 5, 4, 4, 7, 4, 7, 7, 9, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1876, 0.2713, 0.3561,  ..., 0.0056, 0.0108, 0.7483]],\n",
      "\n",
      "        [[0.1036, 0.0313, 0.4258,  ..., 0.2144, 0.1965, 0.9922]],\n",
      "\n",
      "        [[0.0811, 0.0165, 0.3063,  ..., 0.3134, 0.3193, 0.7312]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0738, 0.2289, 0.3989,  ..., 0.2659, 0.0567, 0.5247]],\n",
      "\n",
      "        [[0.3556, 0.3074, 0.0909,  ..., 0.4247, 0.4928, 0.5898]],\n",
      "\n",
      "        [[0.3688, 0.1014, 0.3768,  ..., 0.1829, 0.3818, 0.5756]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 7, 6, 6, 6, 6, 6, 7, 6, 7, 5, 4, 4, 5, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0336, 0.4162, 0.4230,  ..., 0.1625, 0.1136, 0.6131]],\n",
      "\n",
      "        [[0.2325, 0.2725, 0.0825,  ..., 0.2056, 0.4854, 0.8734]],\n",
      "\n",
      "        [[0.1369, 0.3466, 0.1585,  ..., 0.4057, 0.4447, 0.7922]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4263, 0.2118, 0.4388,  ..., 0.4099, 0.1657, 0.6508]],\n",
      "\n",
      "        [[0.4431, 0.3436, 0.4633,  ..., 0.2192, 0.2083, 0.7150]],\n",
      "\n",
      "        [[0.2989, 0.0166, 0.2188,  ..., 0.1870, 0.0838, 0.8400]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 5, 7, 6, 7, 7, 5, 6, 6, 7, 7, 5, 4, 7, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0732, 0.3778, 0.2291,  ..., 0.0919, 0.3731, 0.6741]],\n",
      "\n",
      "        [[0.3259, 0.4416, 0.1316,  ..., 0.0544, 0.0018, 0.9725]],\n",
      "\n",
      "        [[0.4270, 0.3724, 0.1377,  ..., 0.3114, 0.3064, 0.8528]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2696, 0.3583, 0.0013,  ..., 0.1190, 0.0298, 0.6866]],\n",
      "\n",
      "        [[0.2833, 0.2441, 0.2517,  ..., 0.3370, 0.0345, 0.5949]],\n",
      "\n",
      "        [[0.3851, 0.3260, 0.2061,  ..., 0.0693, 0.0367, 0.8336]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 6, 6, 6, 5, 5, 7, 6, 6, 5, 3, 5, 7, 7, 5, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2870, 0.1555, 0.0988,  ..., 0.0125, 0.1918, 0.7323]],\n",
      "\n",
      "        [[0.1231, 0.3815, 0.3083,  ..., 0.3474, 0.1074, 0.8504]],\n",
      "\n",
      "        [[0.2147, 0.1598, 0.3699,  ..., 0.4134, 0.4486, 0.9624]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1538, 0.1931, 0.1753,  ..., 0.1848, 0.4367, 0.5890]],\n",
      "\n",
      "        [[0.1568, 0.4000, 0.0237,  ..., 0.3581, 0.3150, 0.7852]],\n",
      "\n",
      "        [[0.0682, 0.0987, 0.1066,  ..., 0.2560, 0.2047, 0.8163]]],\n",
      "       device='cuda:0')\n",
      "tensor([3, 6, 5, 6, 8, 8, 8, 7, 4, 4, 6, 5, 4, 7, 7, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[2.8936e-01, 2.4508e-01, 3.2841e-01,  ..., 4.2555e-02,\n",
      "          1.8304e-02, 6.3906e-01]],\n",
      "\n",
      "        [[3.6477e-01, 7.4787e-04, 9.5146e-02,  ..., 2.0422e-01,\n",
      "          4.4072e-01, 7.8809e-01]],\n",
      "\n",
      "        [[1.8722e-01, 6.0519e-02, 3.5781e-01,  ..., 2.7664e-01,\n",
      "          4.4956e-01, 8.4390e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.4159e-01, 1.2321e-01, 4.2520e-01,  ..., 4.0776e-01,\n",
      "          2.5794e-02, 6.2940e-01]],\n",
      "\n",
      "        [[2.2007e-01, 2.5494e-01, 1.8984e-01,  ..., 1.9218e-01,\n",
      "          1.5437e-01, 5.5060e-01]],\n",
      "\n",
      "        [[2.3538e-01, 4.5538e-01, 2.3182e-01,  ..., 3.9196e-01,\n",
      "          2.2506e-01, 8.3808e-01]]], device='cuda:0')\n",
      "tensor([5, 5, 6, 4, 6, 7, 5, 3, 3, 6, 6, 6, 4, 4, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2592, 0.4149, 0.0731,  ..., 0.1090, 0.0717, 0.7113]],\n",
      "\n",
      "        [[0.4773, 0.2259, 0.3306,  ..., 0.4857, 0.0497, 0.6120]],\n",
      "\n",
      "        [[0.4595, 0.3574, 0.0603,  ..., 0.2838, 0.4458, 0.6215]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3415, 0.3967, 0.3038,  ..., 0.3876, 0.2886, 0.6718]],\n",
      "\n",
      "        [[0.2446, 0.4373, 0.0334,  ..., 0.4315, 0.1049, 0.9480]],\n",
      "\n",
      "        [[0.3876, 0.0796, 0.0771,  ..., 0.0846, 0.1500, 0.7021]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 6, 7, 6, 8, 8, 6, 7, 7, 7, 6, 7, 6, 6, 6, 4], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1950, 0.2469, 0.2772,  ..., 0.3658, 0.2801, 0.5507]],\n",
      "\n",
      "        [[0.0394, 0.4236, 0.4185,  ..., 0.2289, 0.0039, 0.6790]],\n",
      "\n",
      "        [[0.4877, 0.2233, 0.3652,  ..., 0.1043, 0.2646, 0.6003]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1479, 0.2153, 0.0049,  ..., 0.1245, 0.4101, 0.7990]],\n",
      "\n",
      "        [[0.1628, 0.4951, 0.1720,  ..., 0.4883, 0.0358, 0.9230]],\n",
      "\n",
      "        [[0.3810, 0.4403, 0.0573,  ..., 0.1722, 0.4121, 0.5925]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 5, 4, 7, 6, 5, 5, 3, 4, 6, 6, 8, 5, 5, 6, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3423, 0.4109, 0.4275,  ..., 0.1835, 0.4552, 0.5439]],\n",
      "\n",
      "        [[0.4567, 0.3731, 0.1938,  ..., 0.2085, 0.2771, 0.8208]],\n",
      "\n",
      "        [[0.1114, 0.3502, 0.1053,  ..., 0.3571, 0.3845, 0.6585]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3711, 0.1728, 0.4799,  ..., 0.4360, 0.1923, 0.9945]],\n",
      "\n",
      "        [[0.1366, 0.1297, 0.2114,  ..., 0.3509, 0.3135, 0.9997]],\n",
      "\n",
      "        [[0.1623, 0.3835, 0.3405,  ..., 0.3049, 0.0589, 0.9338]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 5, 5, 7, 6, 6, 5, 5, 6, 7, 6, 4, 6, 5, 7, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2438, 0.3941, 0.0221,  ..., 0.4762, 0.4244, 0.6751]],\n",
      "\n",
      "        [[0.1393, 0.0361, 0.3287,  ..., 0.0203, 0.1025, 0.8718]],\n",
      "\n",
      "        [[0.0204, 0.1391, 0.4865,  ..., 0.3218, 0.1243, 0.8966]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0557, 0.1279, 0.1707,  ..., 0.3735, 0.4279, 0.6244]],\n",
      "\n",
      "        [[0.4585, 0.1671, 0.0234,  ..., 0.1941, 0.2360, 0.8285]],\n",
      "\n",
      "        [[0.3464, 0.4179, 0.0622,  ..., 0.3640, 0.0527, 0.5234]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 4, 4, 4, 7, 6, 6, 7, 5, 8, 4, 5, 7, 7, 4, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2304, 0.0101, 0.0140,  ..., 0.1047, 0.2508, 0.7444]],\n",
      "\n",
      "        [[0.3028, 0.0848, 0.1033,  ..., 0.4532, 0.1087, 0.6090]],\n",
      "\n",
      "        [[0.2320, 0.4343, 0.1320,  ..., 0.3164, 0.3928, 0.5037]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2818, 0.3718, 0.4601,  ..., 0.3654, 0.2873, 0.6656]],\n",
      "\n",
      "        [[0.1818, 0.1608, 0.3746,  ..., 0.3455, 0.2957, 0.6499]],\n",
      "\n",
      "        [[0.1127, 0.3790, 0.1042,  ..., 0.2110, 0.0195, 0.9309]]],\n",
      "       device='cuda:0')\n",
      "tensor([7, 8, 6, 3, 7, 5, 6, 6, 5, 4, 6, 6, 7, 6, 6, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1333, 0.4164, 0.3702,  ..., 0.1596, 0.0791, 0.9220]],\n",
      "\n",
      "        [[0.4558, 0.0271, 0.2520,  ..., 0.3672, 0.4049, 0.6399]],\n",
      "\n",
      "        [[0.0607, 0.3701, 0.3823,  ..., 0.1928, 0.0228, 0.7950]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2537, 0.4739, 0.1402,  ..., 0.0757, 0.3635, 0.7111]],\n",
      "\n",
      "        [[0.0118, 0.2621, 0.3792,  ..., 0.4359, 0.0144, 0.5925]],\n",
      "\n",
      "        [[0.2893, 0.0371, 0.3989,  ..., 0.4339, 0.1087, 0.9959]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 5, 6, 8, 7, 7, 6, 6, 6, 1, 5, 6, 6, 5, 5, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1046, 0.1422, 0.4710,  ..., 0.1925, 0.2966, 0.9649]],\n",
      "\n",
      "        [[0.4096, 0.4319, 0.2671,  ..., 0.4402, 0.1013, 0.7792]],\n",
      "\n",
      "        [[0.4541, 0.3880, 0.4988,  ..., 0.4651, 0.3512, 0.8109]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3599, 0.1069, 0.3197,  ..., 0.4252, 0.4484, 0.5740]],\n",
      "\n",
      "        [[0.0141, 0.2865, 0.1039,  ..., 0.2417, 0.2037, 0.8960]],\n",
      "\n",
      "        [[0.1377, 0.1453, 0.3345,  ..., 0.1476, 0.3037, 0.6476]]],\n",
      "       device='cuda:0')\n",
      "tensor([7, 5, 5, 6, 9, 4, 5, 5, 5, 6, 5, 5, 5, 6, 7, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0572, 0.1887, 0.1116,  ..., 0.3095, 0.3127, 0.5215]],\n",
      "\n",
      "        [[0.4552, 0.3679, 0.4131,  ..., 0.0233, 0.4886, 0.5128]],\n",
      "\n",
      "        [[0.0680, 0.0780, 0.3036,  ..., 0.4381, 0.2606, 0.7688]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2531, 0.0233, 0.4202,  ..., 0.4433, 0.2480, 0.5392]],\n",
      "\n",
      "        [[0.2872, 0.0744, 0.2434,  ..., 0.1619, 0.1630, 0.6984]],\n",
      "\n",
      "        [[0.4935, 0.3991, 0.4846,  ..., 0.0894, 0.0830, 0.7791]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 6, 7, 7, 5, 7, 7, 4, 7, 6, 7, 6, 7, 5, 4, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0393, 0.4637, 0.1937,  ..., 0.3467, 0.3624, 0.6551]],\n",
      "\n",
      "        [[0.4091, 0.2900, 0.0794,  ..., 0.2837, 0.3531, 0.8599]],\n",
      "\n",
      "        [[0.3766, 0.3902, 0.0915,  ..., 0.3416, 0.4480, 0.5279]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1860, 0.0048, 0.2046,  ..., 0.2706, 0.5000, 0.5176]],\n",
      "\n",
      "        [[0.0378, 0.3841, 0.0672,  ..., 0.4069, 0.3743, 0.7767]],\n",
      "\n",
      "        [[0.3719, 0.1132, 0.2683,  ..., 0.0033, 0.1571, 0.8256]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 4, 5, 6, 3, 5, 4, 5, 6, 6, 5, 5, 4, 1, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3363, 0.3115, 0.3622,  ..., 0.1819, 0.4418, 0.7927]],\n",
      "\n",
      "        [[0.4359, 0.4995, 0.2921,  ..., 0.0971, 0.4851, 0.9882]],\n",
      "\n",
      "        [[0.0335, 0.4905, 0.0609,  ..., 0.2777, 0.4985, 0.7528]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2996, 0.1737, 0.0218,  ..., 0.2031, 0.3659, 0.8591]],\n",
      "\n",
      "        [[0.4344, 0.4924, 0.2650,  ..., 0.1316, 0.0556, 0.9397]],\n",
      "\n",
      "        [[0.1214, 0.2281, 0.4815,  ..., 0.3558, 0.3663, 0.9072]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 7, 6, 3, 7, 7, 5, 7, 5, 7, 4, 1, 6, 7, 6, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4377, 0.4183, 0.0303,  ..., 0.1168, 0.4295, 0.7374]],\n",
      "\n",
      "        [[0.1955, 0.4012, 0.4209,  ..., 0.4932, 0.4593, 0.8622]],\n",
      "\n",
      "        [[0.0349, 0.1165, 0.2897,  ..., 0.3886, 0.2418, 0.9238]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1590, 0.0195, 0.1742,  ..., 0.2557, 0.3408, 0.7508]],\n",
      "\n",
      "        [[0.4679, 0.1306, 0.2130,  ..., 0.0628, 0.0881, 0.7618]],\n",
      "\n",
      "        [[0.3429, 0.2383, 0.3463,  ..., 0.3307, 0.4373, 0.8137]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 6, 6, 6, 5, 6, 3, 7, 7, 5, 8, 6, 3, 5, 7, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4283, 0.3197, 0.2396,  ..., 0.2577, 0.4959, 0.6403]],\n",
      "\n",
      "        [[0.0020, 0.4450, 0.1294,  ..., 0.3566, 0.1738, 0.6842]],\n",
      "\n",
      "        [[0.3286, 0.4598, 0.0615,  ..., 0.2706, 0.3371, 0.5790]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1749, 0.0053, 0.2315,  ..., 0.4537, 0.4670, 0.6421]],\n",
      "\n",
      "        [[0.1833, 0.1684, 0.4232,  ..., 0.2143, 0.4831, 0.6603]],\n",
      "\n",
      "        [[0.2197, 0.2007, 0.1765,  ..., 0.1032, 0.4314, 0.9299]]],\n",
      "       device='cuda:0')\n",
      "tensor([7, 6, 6, 7, 6, 7, 6, 8, 6, 7, 4, 6, 6, 3, 7, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2044, 0.1950, 0.3902,  ..., 0.3020, 0.3899, 0.6989]],\n",
      "\n",
      "        [[0.3677, 0.4268, 0.4553,  ..., 0.2054, 0.2839, 0.7871]],\n",
      "\n",
      "        [[0.3772, 0.0915, 0.0240,  ..., 0.0621, 0.4151, 0.8170]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1032, 0.2682, 0.4700,  ..., 0.3191, 0.1964, 0.8593]],\n",
      "\n",
      "        [[0.0030, 0.4455, 0.4465,  ..., 0.1387, 0.1181, 0.7911]],\n",
      "\n",
      "        [[0.4813, 0.3567, 0.2563,  ..., 0.0791, 0.1916, 0.9520]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 6, 5, 5, 7, 6, 7, 6, 4, 6, 6, 4, 6, 5, 4, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0443, 0.4399, 0.0452,  ..., 0.4694, 0.2905, 0.9033]],\n",
      "\n",
      "        [[0.3468, 0.1169, 0.2233,  ..., 0.4256, 0.1251, 0.8707]],\n",
      "\n",
      "        [[0.4219, 0.0273, 0.2110,  ..., 0.3168, 0.4459, 0.9488]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0887, 0.2467, 0.2214,  ..., 0.0620, 0.4021, 0.8295]],\n",
      "\n",
      "        [[0.3512, 0.0252, 0.2189,  ..., 0.0923, 0.3327, 0.9111]],\n",
      "\n",
      "        [[0.4019, 0.4597, 0.2868,  ..., 0.4389, 0.2637, 0.7852]]],\n",
      "       device='cuda:0')\n",
      "tensor([7, 7, 7, 5, 7, 6, 4, 1, 5, 5, 4, 5, 5, 6, 5, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.0160, 0.1203, 0.1675,  ..., 0.4253, 0.4787, 0.6420]],\n",
      "\n",
      "        [[0.0472, 0.0823, 0.2368,  ..., 0.1716, 0.2838, 0.8162]],\n",
      "\n",
      "        [[0.1509, 0.1366, 0.2280,  ..., 0.3992, 0.3953, 0.8414]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0046, 0.4653, 0.2500,  ..., 0.1992, 0.2556, 0.6446]],\n",
      "\n",
      "        [[0.4222, 0.1430, 0.0687,  ..., 0.2913, 0.4421, 0.9271]],\n",
      "\n",
      "        [[0.1917, 0.4086, 0.1499,  ..., 0.0106, 0.4546, 0.6251]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 5, 8, 4, 7, 5, 7, 7, 7, 5, 4, 8, 6, 5, 8, 4], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4372, 0.1065, 0.3242,  ..., 0.4582, 0.0495, 0.9775]],\n",
      "\n",
      "        [[0.4907, 0.2953, 0.2797,  ..., 0.0941, 0.3957, 0.9353]],\n",
      "\n",
      "        [[0.2880, 0.1436, 0.0790,  ..., 0.2638, 0.2305, 0.6049]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0014, 0.2150, 0.0586,  ..., 0.4784, 0.2419, 0.5897]],\n",
      "\n",
      "        [[0.2939, 0.3806, 0.2466,  ..., 0.1075, 0.1122, 0.5983]],\n",
      "\n",
      "        [[0.0581, 0.3901, 0.4516,  ..., 0.3338, 0.0293, 0.5569]]],\n",
      "       device='cuda:0')\n",
      "tensor([3, 7, 7, 6, 6, 6, 5, 6, 5, 4, 5, 4, 6, 6, 5, 4], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3152, 0.1118, 0.4189,  ..., 0.2067, 0.1929, 0.6515]],\n",
      "\n",
      "        [[0.2020, 0.2208, 0.3110,  ..., 0.3821, 0.2024, 0.8186]],\n",
      "\n",
      "        [[0.4352, 0.3539, 0.0671,  ..., 0.4431, 0.1719, 0.6723]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2055, 0.0086, 0.4156,  ..., 0.1197, 0.4735, 0.5547]],\n",
      "\n",
      "        [[0.3073, 0.4697, 0.3412,  ..., 0.4566, 0.4878, 0.9414]],\n",
      "\n",
      "        [[0.3594, 0.3583, 0.4683,  ..., 0.3822, 0.1411, 0.5290]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 5, 6, 4, 6, 5, 5, 5, 7, 7, 5, 3, 5, 5, 5, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.3302, 0.0306, 0.4763,  ..., 0.1395, 0.0020, 0.8290]],\n",
      "\n",
      "        [[0.4529, 0.1715, 0.1972,  ..., 0.4326, 0.3246, 0.8791]],\n",
      "\n",
      "        [[0.3987, 0.3468, 0.0184,  ..., 0.0997, 0.4184, 0.6398]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3455, 0.1195, 0.2317,  ..., 0.1075, 0.4958, 0.9635]],\n",
      "\n",
      "        [[0.3851, 0.0381, 0.2381,  ..., 0.4502, 0.0665, 0.5808]],\n",
      "\n",
      "        [[0.2976, 0.4155, 0.0464,  ..., 0.3217, 0.1570, 0.8697]]],\n",
      "       device='cuda:0')\n",
      "tensor([4, 5, 7, 5, 7, 7, 6, 3, 8, 6, 4, 6, 8, 6, 3, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1282, 0.0036, 0.0169,  ..., 0.3100, 0.2331, 0.5537]],\n",
      "\n",
      "        [[0.0866, 0.0818, 0.1095,  ..., 0.1507, 0.3520, 0.7939]],\n",
      "\n",
      "        [[0.4004, 0.4405, 0.0480,  ..., 0.2603, 0.1092, 0.9300]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2984, 0.2334, 0.1912,  ..., 0.3492, 0.3407, 0.7983]],\n",
      "\n",
      "        [[0.1829, 0.1787, 0.2469,  ..., 0.0451, 0.0799, 0.7832]],\n",
      "\n",
      "        [[0.4140, 0.3841, 0.1683,  ..., 0.4183, 0.4712, 0.8141]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 7, 5, 5, 6, 7, 4, 3, 6, 3, 6, 8, 4, 5, 7, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2046, 0.0624, 0.1432,  ..., 0.0192, 0.0878, 0.9399]],\n",
      "\n",
      "        [[0.4762, 0.4325, 0.0365,  ..., 0.4029, 0.3379, 0.7141]],\n",
      "\n",
      "        [[0.3060, 0.0622, 0.4785,  ..., 0.0996, 0.1308, 0.5689]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2495, 0.2672, 0.4328,  ..., 0.4081, 0.0618, 0.8011]],\n",
      "\n",
      "        [[0.0187, 0.2049, 0.2988,  ..., 0.0262, 0.0891, 0.6949]],\n",
      "\n",
      "        [[0.4871, 0.1741, 0.0517,  ..., 0.4834, 0.3402, 0.6251]]],\n",
      "       device='cuda:0')\n",
      "tensor([6, 6, 6, 5, 6, 6, 4, 5, 4, 5, 5, 5, 6, 8, 6, 7], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.2664, 0.4501, 0.3553,  ..., 0.1239, 0.4850, 0.8346]],\n",
      "\n",
      "        [[0.3213, 0.0254, 0.0050,  ..., 0.0355, 0.3665, 0.7390]],\n",
      "\n",
      "        [[0.3102, 0.3660, 0.3288,  ..., 0.4515, 0.2788, 0.8570]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3457, 0.3258, 0.2268,  ..., 0.3162, 0.3585, 0.6846]],\n",
      "\n",
      "        [[0.3879, 0.3185, 0.4834,  ..., 0.3103, 0.2993, 0.8079]],\n",
      "\n",
      "        [[0.4049, 0.3016, 0.1552,  ..., 0.2350, 0.4734, 0.9639]]],\n",
      "       device='cuda:0')\n",
      "tensor([8, 6, 5, 5, 5, 2, 4, 4, 7, 7, 5, 6, 3, 6, 7, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1442, 0.3781, 0.2965,  ..., 0.3437, 0.4824, 0.8869]],\n",
      "\n",
      "        [[0.2930, 0.3046, 0.1436,  ..., 0.1620, 0.4483, 0.8795]],\n",
      "\n",
      "        [[0.2053, 0.2556, 0.1472,  ..., 0.4133, 0.2007, 0.9706]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3407, 0.2360, 0.1117,  ..., 0.0408, 0.0369, 0.8748]],\n",
      "\n",
      "        [[0.3805, 0.2463, 0.4205,  ..., 0.0018, 0.1585, 0.9919]],\n",
      "\n",
      "        [[0.4420, 0.1179, 0.1954,  ..., 0.4435, 0.4089, 0.9964]]],\n",
      "       device='cuda:0')\n",
      "tensor([5, 6, 7, 5, 7, 5, 6, 5, 6, 7, 4, 6, 4, 4, 5, 6], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.4843, 0.0500, 0.3105,  ..., 0.4101, 0.4891, 0.7651]],\n",
      "\n",
      "        [[0.3775, 0.1797, 0.3670,  ..., 0.4636, 0.4262, 0.5241]],\n",
      "\n",
      "        [[0.0226, 0.2787, 0.4408,  ..., 0.4201, 0.3014, 0.6504]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.2093, 0.1007, 0.1709,  ..., 0.3762, 0.2923, 0.7049]],\n",
      "\n",
      "        [[0.1990, 0.4842, 0.2675,  ..., 0.1613, 0.4886, 0.8652]],\n",
      "\n",
      "        [[0.4747, 0.2194, 0.4361,  ..., 0.3605, 0.3281, 0.8136]]],\n",
      "       device='cuda:0')\n",
      "tensor([7, 6, 6, 7, 7, 8, 6, 6, 3, 4, 1, 3, 7, 7, 7, 5], device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n",
      "tensor([[[0.1549, 0.4695, 0.4988,  ..., 0.2068, 0.4642, 0.9440]],\n",
      "\n",
      "        [[0.4219, 0.0413, 0.4420,  ..., 0.2658, 0.1619, 0.6584]],\n",
      "\n",
      "        [[0.4218, 0.4755, 0.1280,  ..., 0.4507, 0.3446, 0.5029]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0168, 0.0546, 0.2386,  ..., 0.3664, 0.4869, 0.5877]],\n",
      "\n",
      "        [[0.4775, 0.2750, 0.1292,  ..., 0.4901, 0.0942, 0.6208]],\n",
      "\n",
      "        [[0.4010, 0.0351, 0.3932,  ..., 0.3078, 0.4541, 0.5327]]],\n",
      "       device='cuda:0')\n",
      "tensor([ 6,  4,  6,  6,  7,  6,  4,  3, 10,  6,  6,  6,  5,  5,  4,  5],\n",
      "       device='cuda:0')\n",
      "torch.Size([16, 1, 1312])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\DEV\\projet-de-recherche-master-is\\2_Exp_molecules\\1_selfies_diffusion_1D.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(training_mols\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(mols_classes\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m diffusion(training_mols, classes \u001b[39m=\u001b[39;49m mols_classes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\charb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\DEV\\projet-de-recherche-master-is\\2_Exp_molecules\\1_selfies_diffusion_1D.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=799'>800</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, (b,), device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39mlong()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=801'>802</a>\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(img)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=802'>803</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_losses(img, t, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\DEV\\projet-de-recherche-master-is\\2_Exp_molecules\\1_selfies_diffusion_1D.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=772'>773</a>\u001b[0m         x_self_cond\u001b[39m.\u001b[39mdetach_()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=774'>775</a>\u001b[0m \u001b[39m# predict and take gradient step\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=775'>776</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=776'>777</a>\u001b[0m \u001b[39m#model_out = self.model(x, t, x_self_cond)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=777'>778</a>\u001b[0m model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, t, classes, x_self_cond)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=779'>780</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpred_noise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=780'>781</a>\u001b[0m     target \u001b[39m=\u001b[39m noise\n",
      "File \u001b[1;32mc:\\Users\\charb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\DEV\\projet-de-recherche-master-is\\2_Exp_molecules\\1_selfies_diffusion_1D.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=428'>429</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x_self_cond, x), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=430'>431</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_conv(x)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=431'>432</a>\u001b[0m r \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=433'>434</a>\u001b[0m t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_mlp(time)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/2_Exp_molecules/1_selfies_diffusion_1D.ipynb#X21sZmlsZQ%3D%3D?line=435'>436</a>\u001b[0m h \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      optimizer.zero_grad()\n",
    "      training_mols = batch[0].to(device) # normalized from 0 to 1\n",
    "      mols_classes = batch[1].to(device)    # say 10 classes\n",
    "\n",
    "      loss = diffusion(training_mols, classes = mols_classes)\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      # save generated mols\n",
    "      if step == 0 and (epoch == 0 or epoch % save_and_sample_every == 0):\n",
    "        try:\n",
    "          nb_mols = 30\n",
    "          samples_classes = torch.tensor([i // (nb_mols//3) for i in range(nb_mols)]).to(device)\n",
    "          # conditional\n",
    "          all_continous_mols = torch.squeeze(diffusion.sample(samples_classes, cond_scale = 6.))\n",
    "          recalculate_selfies = mols_continous_to_selfies(all_continous_mols, selfies_alphabet, largest_selfie_len, int_mol, dequantized_onehots_min, dequantized_onehots_max)\n",
    "          mols, valid_selfies_list, valid_count = selfies_to_mols(recalculate_selfies)\n",
    "          print('%.2f' % (valid_count / len(recalculate_selfies)*100),  '% of generated samples are valid molecules.')\n",
    "\n",
    "          # Similarity calculation\n",
    "          print(\"Similarity\")\n",
    "          print(\"----------\")\n",
    "          for mol_to_check in mols[:9]:\n",
    "            print(\"Smiles:\", Chem.MolToSmiles(mol_to_check))\n",
    "            tanimoto_scores = tanimoto_similarity(original_mols, mol_to_check)\n",
    "            print(\"Top3 similarity score:\")\n",
    "            for idx, ts in tanimoto_scores[:3]:\n",
    "                print(round(ts, 3))\n",
    "          img = Chem.Draw.MolsToGridImage(mols, molsPerRow=3, subImgSize=(200,200), returnPNG=False)\n",
    "          img.save(str(results_folder / f'mol1D-conditional-{epoch}-{step}.png'))\n",
    "        except Exception:\n",
    "          pass  \n",
    "        #print(recalculate_selfies[1])\n",
    "      #  save_image(all_images.view(all_images.shape[0],all_images.shape[1],32,32), str(results_folder / f'test1D1000-unconditional-{epoch}-{step}.png'), nrow = 6)\n",
    "        # conditionnal\n",
    "      #  all_images = diffusion.sample(images_classes, cond_scale = 6.)     \n",
    "      #  save_image(all_images.view(all_images.shape[0],all_images.shape[1],32,32), str(results_folder / f'test1D1000-conditionnal-{epoch}-{step}.png'), nrow = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_images = diffusion.sample(\n",
    "    classes = image_classes,\n",
    "    cond_scale = 6.                # condition scaling, anything greater than 1 strengthens the classifier free guidance. reportedly 3-8 is good empirically\n",
    ")\n",
    "\n",
    "sampled_images.shape # (8, 3, 128, 128)\n",
    "\n",
    "# interpolation\n",
    "\n",
    "interpolate_out = diffusion.interpolate(\n",
    "    training_images[:1],\n",
    "    training_images[:1],\n",
    "    image_classes[:1]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
