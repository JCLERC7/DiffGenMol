{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from denoising_diffusion_pytorch.version import __version__\n",
    "\n",
    "# constants\n",
    "\n",
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "\n",
    "# helpers functions\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def has_int_squareroot(num):\n",
    "    return (math.sqrt(num) ** 2) == num\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "def convert_image_to_fn(img_type, image):\n",
    "    if image.mode != img_type:\n",
    "        return image.convert(img_type)\n",
    "    return image\n",
    "\n",
    "# normalization functions\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "# data\n",
    "\n",
    "class Dataset1D(Dataset):\n",
    "    def __init__(self, tensor: Tensor):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor.clone()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx].clone()\n",
    "\n",
    "# small helper modules\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv1d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out = None):\n",
    "    return nn.Conv1d(dim, default(dim_out, dim), 4, 2, 1)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "# sinusoidal positional embeds\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "# building block modules\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv1d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale        \n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c n -> b (h c) n', h = self.heads)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv1d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, n = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -> b h c n', h = self.heads), qkv)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h d j -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b (h d) n')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# model\n",
    "\n",
    "class Unet1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim = None,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels = 3,\n",
    "        self_condition = False,\n",
    "        resnet_block_groups = 8,\n",
    "        learned_variance = False,\n",
    "        learned_sinusoidal_cond = False,\n",
    "        random_fourier_features = False,\n",
    "        learned_sinusoidal_dim = 16,\n",
    "        sinusoidal_pos_emb_theta = 10000,\n",
    "        attn_dim_head = 32,\n",
    "        attn_heads = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv1d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n",
    "\n",
    "        if self.random_or_learned_sinusoidal_cond:\n",
    "            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n",
    "            fourier_dim = learned_sinusoidal_dim + 1\n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)\n",
    "            fourier_dim = dim\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n",
    "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv1d(dim_in, dim_out, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim, dim_head = attn_dim_head, heads = attn_heads)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n",
    "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv1d(dim_out, dim_in, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n",
    "        self.final_conv = nn.Conv1d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond = None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim = 1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "# gaussian diffusion trainer class\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype = torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "class GaussianDiffusion1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        seq_length,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        objective = 'pred_noise',\n",
    "        beta_schedule = 'cosine',\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            betas = linear_beta_schedule(timesteps)\n",
    "        elif beta_schedule == 'cosine':\n",
    "            betas = cosine_beta_schedule(timesteps)\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # calculate loss weight\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            loss_weight = torch.ones_like(snr)\n",
    "        elif objective == 'pred_x0':\n",
    "            loss_weight = snr\n",
    "        elif objective == 'pred_v':\n",
    "            loss_weight = snr / (snr + 1)\n",
    "\n",
    "        register_buffer('loss_weight', loss_weight)\n",
    "\n",
    "        # whether to autonormalize\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n",
    "        model_output = self.model(x, t, x_self_cond)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t: int, x_self_cond = None, clip_denoised = True):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = clip_denoised)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape):\n",
    "        batch, device = shape[0], self.betas.device\n",
    "\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond)\n",
    "\n",
    "        img = self.unnormalize(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ddim_sample(self, shape, clip_denoised = True):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = clip_denoised)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "        img = self.unnormalize(img)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size = 16):\n",
    "        seq_length, channels = self.seq_length, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, seq_length))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, noise = None):\n",
    "        b, c, n = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model(x, t, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, n, device, seq_length, = *img.shape, img.device, self.seq_length\n",
    "        assert n == seq_length, f'seq length must be {seq_length}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3774, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet1D, GaussianDiffusion1D, Trainer1D, Dataset1D\n",
    "\n",
    "model = Unet1D(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    channels = 1\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    model,\n",
    "    seq_length = 40,\n",
    "    timesteps = 1000,\n",
    "    objective = 'pred_v'\n",
    ").cuda()\n",
    "\n",
    "training_seq = torch.rand(64, 1, 40).cuda() # features are normalized from 0 to 1\n",
    "dataset = Dataset1D(training_seq)  # this is just an example, but you can formulate your own Dataset and pass it into the `Trainer1D` below\n",
    "\n",
    "loss = diffusion(training_seq)\n",
    "loss.backward()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5663228631019592\n",
      "Loss: 0.6100636720657349\n",
      "Loss: 1.3967185020446777\n",
      "Loss: 0.3394322395324707\n",
      "Loss: 0.5314361453056335\n",
      "Loss: 0.40555423498153687\n",
      "Loss: 0.716754138469696\n",
      "Loss: 0.3274933099746704\n",
      "Loss: 0.47709691524505615\n",
      "Loss: 0.5166071057319641\n",
      "Loss: 0.34838178753852844\n",
      "Loss: 0.42419183254241943\n",
      "Loss: 0.313870906829834\n",
      "Loss: 1.0780959129333496\n",
      "Loss: 0.3640543222427368\n",
      "Loss: 0.44372808933258057\n",
      "Loss: 0.35837674140930176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\DEV\\projet-de-recherche-master-is\\1_Exp_images\\1_Notebook\\4_test_diffusion_1D.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/1_Exp_images/1_Notebook/4_test_diffusion_1D.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m diffusion(training_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/1_Exp_images/1_Notebook/4_test_diffusion_1D.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/1_Exp_images/1_Notebook/4_test_diffusion_1D.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/DEV/projet-de-recherche-master-is/1_Exp_images/1_Notebook/4_test_diffusion_1D.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\charb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\charb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    #training_images = batch[0].cuda() # images are normalized from 0 to 1\n",
    "    training_images = torch.rand(64, 1, 40).cuda()\n",
    "    \n",
    "    loss = diffusion(training_images)\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
